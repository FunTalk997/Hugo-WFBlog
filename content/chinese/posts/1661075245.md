---
title: "HDFS的读写流程"
date: 2022-08-22T01:47:25+08:00

# format for string: "xxxx-xx-xx"
# lastmod: "2022-10-01"

# set false when you want the post publish
draft: false
# one category: ["category-1"] 
# more categories: ["category-1", "category-2", ...]
categories: ['Hadoop']
# refer to categories
tags: ['Hadoop','HDFS']
# seires
series: ['大数据']
# Top image for the post
image: "/images/postImg/hadoop.png"
# Hide from home page
hideFromHomePage: false
---

# HDFS的读流程

![photo](/images/posts/1661075245/image1.png)

1）客户端调用 DistributeFileSystem 对象的 open() 方法打开要读取的文件。
2）DistributeFileSystem 向 NameNode 发起 RPC 调用，得到文件的数据块信息，返回数据节点列表。对于每个数据块，NameNode 返回该数据块的 DataNode 地址。
3）DistributeFileSystem 返回一个 FSDataInputStrem 对象给客户端，客户端调用 FSDataInputStrem 对象的 read() 方法开始读取数据。
4）通过对数据流反复调用 read() 方法，把数据从数据节点传输到客户端。
5）当一个数据块的数据读取完毕时，DFSInputStrem 对象关闭与此数据节点的连接，连接文件的下一个数据块的最近数据节点。
6）当文件的数据读取完时，客户端调用 DistributeFileSystem 对象的 close() 方法关闭文件输入流对象。

# HDFS的写流程

![photo](/images/posts/1661075245/image2.png)

1）客户端调用 DistributeFileSystem 对象的 create() 方法创建一个文件输出流对象。
2）DistributeFileSystem 向 NameNode 发起 RPC 调用，NameNode 检查该文件是否以及存在，以及客户端是否有权限新建文件。
3）DistributeFileSystem 返回一个 FSDataOutputStrem 对象给客户端，客户端调用 FSDataOutputStrem 对象的 write() 方法写入数据，数据先被写入到缓冲区，再被切分成一个个数据包。
4）每个数据包被发送到由 NameNode 分配的一组数据节点的一个节点上，在这组数据节点组成的管道上依次传输数据包。
5）管道上的数据节点按方向顺序返回确认信息，最终由管道上的第一个数据节点将整条管道的确认信息返回给客户端。
6）客户端完成写入，调用 close() 方法关闭文件输出流对象。
7）通知 NameNode 文件写入成功。