---
title: "Spark UDF/UDAF"
date: 2022-07-22T00:10:33+08:00

# format for string: "xxxx-xx-xx"
# lastmod: "2022-10-01"

# set false when you want the post publish
draft: false
# one category: ["category-1"] 
# more categories: ["category-1", "category-2", ...]
categories: ['Spark']
# refer to categories
tags: ['Spark','UDF','UDAF']
# seires
series: ['Big Data']
# Top image for the post
image: "/images/postImg/spark.png"
# Hide from home page
hideFromHomePage: false
---

# UDF

## Create DataFrame

```scala
scala> val df = spark.read.json("data/user.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]
```

## Register UDF

```scala
scala> spark.udf.register("addName", (x: String) => "Name:" + x)
res9: org.apache.spark.sql.expressions.UserDefinedFunction =
UserDefinedFunction(<function1>, StringType, Some(List(StringType)))
```

## Create temporary table

```scala
scala> df.createOrReplaceTempView("people")
```

## Apply UDF

```scala
scala> spark.sql("SELECT addName(name), age FROM people").show()
```

# UDAF

Both strong-typed Datasets and weak-typed DataFrames provide built-in aggregation functions such as count(), countDistinct(), avg(), max(), min(). However, users can also define their own custom aggregation functions. To implement a user-defined weak-typed aggregation function, you can inherit from UserDefinedAggregateFunction. Starting from Spark 3.0, UserDefinedAggregateFunction is no longer recommended, and you can use the strong-typed Aggregator as a unified way to define custom aggregation functions.

Requirement: Calculate average salary
There are multiple ways to implement a single requirement.

## Implementation - RDD

```scala
val conf: SparkConf = new SparkConf().setAppName("app").setMaster("local[*]")
val sc: SparkContext = new SparkContext(conf)
val res: (Int, Int) = sc.makeRDD(List(("zhangsan", 20), ("lisi", 30), ("wangw", 40))).map {
  case (name, age) => {
    (age, 1)
  }
}.reduce {
  (t1, t2) => {
    (t1._1 + t2._1, t1._2 + t2._2)
  }
}
println(res._1 / res._2)
// Close connection
sc.stop()
```

## Implementation - Accumulator

```scala
class MyAC extends AccumulatorV2[Int, Int] {
  var sum: Int = 0
  var count: Int = 0
  override def isZero: Boolean = {
    return sum == 0 && count == 0
  }
  override def copy(): AccumulatorV2[Int, Int] = {
    val newMyAc = new MyAC
    newMyAc.sum = this.sum
    newMyAc.count = this.count
    newMyAc
  }
  override def reset(): Unit = {
    sum = 0
    count = 0
  }
  override def add(v: Int): Unit = {
    sum += v
    count += 1
  }
  override def merge(other: AccumulatorV2[Int, Int]): Unit = {
    other match {
      case o: MyAC => {
        sum += o.sum
        count += o.count
      }
      case _ =>
    }
  }
  override def value: Int = sum / count
}
```

## Implementation - UDAF (Weak-typed)

```scala
/*
Define a class that extends UserDefinedAggregateFunction and override its methods.
*/
class MyAveragUDAF extends UserDefinedAggregateFunction {
  // Data type of the input parameter of the aggregation function
  def inputSchema: StructType = StructType(Array(StructField("age", IntegerType)))

  // Data type of the value stored in the aggregation buffer (age, count)
  def bufferSchema: StructType = {
    StructType(Array(StructField("sum", LongType), StructField("count", LongType)))
  }

  // Data type of the return value
  def dataType: DataType = DoubleType

  // Stability: Whether the function always returns the same output for the same input.
  def deterministic: Boolean = true

  // Initialize the buffer
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    // Store the sum of ages
    buffer(0) = 0L
    // Store the count of ages
    buffer(1) = 0L
  }

  // Update the buffer with new input
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0)) {
      buffer(0) = buffer.getLong(0) + input.getInt(0)
      buffer(1) = buffer.getLong(1) + 1
    }
  }

  // Merge two buffers
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }

  // Calculate the final result
  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)
}
...
// Create the UDAF
var myAverage = new MyAveragUDAF
// Register the UDAF in Spark
spark.udf.register("avgAge", myAverage)
spark.sql("SELECT avgAge(age) FROM user").show()
```

## Implementation - UDAF (Strong-typed)

```scala
// Input data type
case class User01(username: String, age: Long)
// Buffer type
case class AgeBuffer(var sum: Long, var count: Long)

/**
 * Define a class that extends org.apache.spark.sql.expressions.Aggregator
 * Override the methods in the class.
 */
class MyAveragUDAF1 extends Aggregator[User01, AgeBuffer, Double] {
  override def zero: AgeBuffer = {
    AgeBuffer(0L, 0L)
  }

  override def reduce(b: AgeBuffer, a: User01): AgeBuffer = {
    b.sum = b.sum + a.age
    b.count = b.count + 1
    b
  }

  override def merge(b1: AgeBuffer, b2: AgeBuffer): AgeBuffer = {
    b1.sum = b1.sum + b2.sum
    b1.count = b1.count + b2.count
    b1
  }

  override def finish(buff: AgeBuffer): Double = {
    buff.sum.toDouble / buff.count
  }

  // Default encoder for DataSet, used for serialization, fixed syntax
  // For custom types, it depends on the type being used
  override def bufferEncoder: Encoder[AgeBuffer] = {
    Encoders.product
  }

  override def outputEncoder: Encoder[Double] = {
    Encoders.scalaDouble
  }
}
...
// Convert DataFrame to DataSet
val ds: Dataset[User01] = df.as[User01]
// Create the UDAF
var myAgeUdaf1 = new MyAveragUDAF1
// Convert the UDAF to a column for querying
val col: TypedColumn[User01, Double] = myAgeUdaf1.toColumn
// Query
ds.select(col).show()
```

Starting from Spark 3.0, strong-typed Aggregator is recommended to be used instead of UserDefinedAggregateFunction.

```scala
// TODO Create UDAF function
val udaf = new MyAvgAgeUDAF
// TODO Register in SparkSQL
spark.udf.register("avgAge", functions.udaf(udaf))
// TODO Use the aggregate function in SQL
// Define a custom aggregation function for users
spark.sql("SELECT avgAge(age) FROM user").show
// **************************************************
case class Buff(var sum: Long, var cnt: Long)
// totalage, count
class MyAvgAgeUDAF extends Aggregator[Long, Buff, Double] {
  override def zero: Buff = Buff(0, 0)

  override def reduce(b: Buff, a: Long): Buff = {
    b.sum += a
    b.cnt += 1
    b
  }

  override def merge(b1: Buff, b2: Buff): Buff = {
    b1.sum += b2.sum
    b1.cnt += b2.cnt
    b1
  }

  override def finish(reduction: Buff): Double = {
    reduction.sum.toDouble / reduction.cnt
  }

  override def bufferEncoder: Encoder[Buff] = Encoders.product

  override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}
```