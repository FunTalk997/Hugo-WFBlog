---
title: "SparkSQL Data loading and saving
"
date: 2022-07-21T22:54:47+08:00

# format for string: "xxxx-xx-xx"
# lastmod: "2022-10-01"

# set false when you want the post publish
draft: false
# one category: ["category-1"] 
# more categories: ["category-1", "category-2", ...]
categories: ['Spark']
# refer to categories
tags: ['Spark','SparkSQL']
# seires
series: ['Big Data']
# Top image for the post
image: "/images/postImg/spark.png"
# Hide from home page
hideFromHomePage: false
---

# Common loading and saving methods

SparkSQL provides a common way to save data and load it. Common here refers to the use of the same API, according to different parameters to read and save different formats of data, SparkSQL default read and save file format is parquet

## Load data

spark.read.load is a common way to load data

```scala
scala> spark.read.
csv	format jdbc json load option options orc parquet schema table text textFile
```

If you read data in different formats, you can set different data formats

```scala
scala> spark.read.format("…")[.option("…")].load("…")
```

- format("…" ) : Specifies the data type to load, including "csv", "jdbc", "json", "orc", "parquet", and "textFile".
- load("…" ) : In the "csv", "jdbc", "json", "orc", "parquet" and "textFile" formats, you need to pass in the path to load the data.
- option("…" ) : In the "jdbc" format need to pass the JDBC parameters, url, user, password and dbtable we used the read API first load the file to the DataFrame and then query, in fact, we can also directly on the file to query: File format.\'file path\'

```scala
scala>spark.sql("select * from json.`/opt/module/data/user.json`").show
```

## Save the data

df.write.save is a common way to save data

```scala
scala>df.write.
csv jdbc json orc parquet textFile… …
```

If you save data in different formats, you can set different data formats

```scala
scala>df.write.format("…")[.option("…")].save("…")
```

- format("…" Specifies the data type to save, including "csv", "jdbc", "json", "orc", "parquet", and "textFile".
- save ("…" ) : In the "csv", "orc", "parquet" and "textFile" formats you need to pass in the path to save the data.
- option("…" ) : In the "jdbc" format, JDBC parameters need to be passed, url, user, password and dbtable saving operations can use SaveMode, which is used to indicate how to process data, using the mode() method to set. It is important to note that these savemodes are unlocked and not atomic operations.

SaveMode is an enumeration class whose constants include:

| Scala/Java                      | Any Language     | Meaning                    |
| ------------------------------- | ---------------- | -------------------------- |
| SaveMode.ErrorIfExists(default) | "error"(default) | Throw an exception if the file already exists |
| SaveMode.Append                 | "append"         | Append if the file already exists     |
| SaveMode.Overwrite              | "overwrite"      | Overwrite the file if it already exists     |
| SaveMode.Ignore                 | "ignore"         | Ignore the file if it already exists     |

```scala
df.write.mode("append").json("/opt/module/data/output")
```

# Parquet

The default data source for Spark SQL is Parquet format. Parquet is a columnar storage format that can efficiently store nested data.
When the data source is a Parquet file, Spark SQL can easily perform all operations without the need to use format. Modify the configuration items spark. SQL. Sources. Default, can change the default data format.

## Load data

```scala
scala> val df = spark.read.load("examples/src/main/resources/users.parquet")
scala> df.show
```

## Save the data

```scala
scala> var df = spark.read.json("/opt/module/data/input/people.json")
// Save in parquet format
scala> df.write.mode("append").save("/opt/module/data/output")
```

# JSON

Spark SQL can automatically infer the structure of the JSON data set and load it into a Dataset[Row]. You can use SparkSession.read.json() to load the JSON file.
Note: The JSON file read by Spark is not a traditional JSON file, and each line should be a JSON string. The formula is as follows:

```json
{"name":"Michael"}
{"name":"Andy"， "age":30}
[{"name":"Justin"， "age":19},{"name":"Justin"， "age":19}]
```

## Import an implicit line feed

`import spark.implicits._`

## Load the JSON file

`val path = "/opt/module/spark-local/people.json"`
`val peopleDF = spark.read.json(path)`

## Create a temporary table

`peopleDF.createOrReplaceTempView("people")`

## Data query

```json
val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")
teenagerNamesDF.show()
+------+
| name|
+------+
|Justin|
+------+
```

# CSV

Spark SQL can configure the list information of a CSV file, read a CSV file, and set the first row of the CSV file as a data column

```scala
spark.read.format("csv").option("sep", ";").option("inferSchema", "true").option("header", "true").load("data/user.csv")
```

# MySQL

Spark SQL can create a DataFrame by reading data from a relational database through JDBC. After a series of calculations on the DataFrame, data can be written back to the relational database. If spark-shell is used, you can specify the related database driver path or place the related database driver in the spark classpath when starting the shell.
'bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar'
IDEA operates on MySQL through JDBC

## Import dependencies

```xml
<dependency>
  <groupId>mysql</groupId>
  <artifactId>mysql-connector-java</artifactId>
  <version>5.1.27</version>
</dependency>
```

## 读取数据

```scala
val conf: SparkConf = new
SparkConf().setMaster("local[*]").setAppName("SparkSQL")
// Create a SparkSession object
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._
// Method 1: Common load method read
spark.read.format("jdbc")
  .option("url", "jdbc:mysql://linux1:3306/spark-sql")
  .option("driver", "com.mysql.jdbc.Driver")
  .option("user", "root")
  .option("password", "123123")
  .option("dbtable", "user")
  .load().show
// Method 2: The common load method reads another form of the parameter
spark.read.format("jdbc")
  .options(Map("url"->"jdbc:mysql://linux1:3306/spark-sql?user=root&password=
  123123","dbtable"->"user","driver"->"com.mysql.jdbc.Driver")).load().show
// Mode 3: Read data using the jdbc method
val props: Properties = new Properties()
props.setProperty("user", "root")
props.setProperty("password", "123123")
val df: DataFrame = spark.read.jdbc("jdbc:mysql://linux1:3306/spark-sql",
                                    "user", props)
df.show
// Release resources
spark.stop()
```

## Write data

```scala
case class User2(name: String, age: Long)
。。。
val conf: SparkConf = new
SparkConf().setMaster("local[*]").setAppName("SparkSQL")
// Create a SparkSession object
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
import spark.implicits._
val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2("lisi", 20),
User2("zs", 30)))
val ds: Dataset[User2] = rdd.toDS
// Mode 1: Common format Specifies the write type
ds.write
 .format("jdbc")
 .option("url", "jdbc:mysql://linux1:3306/spark-sql")
 .option("user", "root")
 .option("password", "123123")
 .option("dbtable", "user")
 .mode(SaveMode.Append)
 .save()
// Method 2: Use jdbc
val props: Properties = new Properties()
props.setProperty("user", "root")
props.setProperty("password", "123123")
ds.write.mode(SaveMode.Append).jdbc("jdbc:mysql://linux1:3306/spark-sql",
"user", props)
// Release resources
spark.stop()
```

# Hive

Apache Hive is the SQL engine on Hadoop. Spark SQL can be compiled with or without Hive support. Spark SQL, including Hive support, supports Hive table access, UDF (user-defined function), and Hive query language (HiveQL/HQL). It is important to emphasize that Hive does not need to be installed in advance to include Hive libraries in Spark SQL. In general, it is best to introduce Hive support when compiling Spark SQL so that these features can be used. If you downloaded the binary version of Spark, it should have added Hive support when it was compiled.
To connect Spark SQL to a deployed hive, you must copy hive-site.xml into Spark's configuration file directory ($SPARK_HOME/conf). Spark SQL can run even if Hive is not deployed. Note that if you don't have Hive deployed, Spark SQL will create its own Hive metadata repository, called metastore_db, in the current working directory. Also, if you try to CREATE a TABLE using the CREATE TABLE (not CREATE EXTERNAL TABLE) statement in HiveQL, These tables will be placed in the /user/hive/warehouse directory of your presumed file system (the default file system is HDFS if you have HDFs-site.xml in your classpath, otherwise it is the local file system). spark-shell supports Hive by default. It is not supported by default in the code and needs to be specified manually (add a parameter).

## Embedded Hive

If you use Hive embedded in Spark, you do not need to do anything.
Hive metadata is stored in derby, with the default repository address :$SPARK_HOME/spark-warehouse

```scala
scala> spark.sql("show tables").show
。。。
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+
scala> spark.sql("create table aa(id int)")
。。。
scala> spark.sql("show tables").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default| aa| false|
+--------+---------+-----------+
```

Load local data to the table

```scala
scala> spark.sql("load data local inpath 'input/ids.txt' into table aa")
。。。
scala> spark.sql("select * from aa").show
+---+
| id|
+---+
| 1|
| 2|
| 3|
| 4|
+---+
```

## External Hive

To connect the deployed Hive, perform the following steps:

- Spark to take over hive, copy hive-site.xml to the conf/ directory
- copy Mysql drivers to the jars/ directory
- If the hdfs cannot be accessed, copy core-site.xml and hdfs-site.xml to the conf/ directory
- Restart spark-shell

```scala
scala> spark.sql("show tables").show
20/04/25 22:05:14 WARN ObjectStore: Failed to get database global_temp, returning
NoSuchObjectException
+--------+--------------------+-----------+
|database| tableName|isTemporary|
+--------+--------------------+-----------+
| default| emp| false|
| default|hive_hbase_emp_table| false|
| default| relevance_hbase_emp| false|
| default| staff_hive| false|
| default| ttt| false|
| default| user_visit_action| false|
+--
```

## Run the Spark SQL CLI

The Spark SQL CLI enables you to run the Hive metadata service locally and perform query tasks on the command line. Run the following command in the Spark directory to start the Spark SQL CLI and execute SQL statements, which is similar to a Hive window
`bin/spark-sql`

## Run Spark beeline

Spark Thrift Server is a Thrift service implemented by the Spark community based on HiveServer## Designed for seamless compatibility with HiveServer## The interfaces and protocols of Spark Thrift Server are the same as those of HiveServer## After we deploy Spark Thrift Server, You can use the beeline of hive to access the Spark Thrift Server and execute related statements. Spark Thrift Server is only intended to replace HiveServer2, so it can still interact with Hive Metastore and obtain hive metadata.
To connect to the Thrift Server, you need to perform the following steps:

- Spark to take over hive, copy hive-site.xml to the conf/ directory
- copy Mysql drivers to the jars/ directory
- If the hdfs cannot be accessed, copy core-site.xml and hdfs-site.xml to the conf/ directory
- Start the Thrift Server

`sbin/start-thriftserver.sh`

- Use beeline to connect to the Thrift Server

`bin/beeline -u jdbc:hive2://localhost:10000 -n root`
![photo](/images/posts/1658386487/image2.png)

## Code operation Hive
  a. Import dependencies

```xml
<dependency>
 <groupId>org.apache.spark</groupId>
 <artifactId>spark-hive_2.12</artifactId>
 <version>3.4.0</version>
</dependency>
<dependency>
 <groupId>org.apache.hive</groupId>
 <artifactId>hive-exec</artifactId>
 <version>1.2.1</version>
</dependency>
<dependency>
 <groupId>mysql</groupId>
 <artifactId>mysql-connector-java</artifactId>
 <version>5.1.27</version>
</dependency>
```

  b. Copy the hive-site.xml file to the resources directory of the project

```scala
// Create a SparkSession
val spark: SparkSession = SparkSession
.builder()
.enableHiveSupport()
.master("local[*]")
.appName("sql")
.getOrCreate()
```

Note: By default, the database is created in the local warehouse in the development tool, and the address of the database warehouse is modified by parameters:
`config("spark.sql.warehouse.dir", "hdfs://localhost:8020/user/hive/warehouse")`
When performing an operation, the following error occurs:
![photo](/images/posts/1658386487/image2.png)
The following code can be added to the front of the code:
`System.setProperty("HADOOP_USER_NAME", "root")`
Change root here to your own hadoop user name

