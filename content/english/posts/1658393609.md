---
title: "SparkCore Basic programming"
date: 2022-07-22T00:53:29+08:00

# format for string: "xxxx-xx-xx"
# lastmod: "2022-10-01"

# set false when you want the post publish
draft: false
# one category: ["category-1"] 
# more categories: ["category-1", "category-2", ...]
categories: ['Spark']
# refer to categories
tags: ['Spark','SparkCore']
# seires
series: ['Big Data']
# Top image for the post
image: "/images/postImg/spark.png"
# Hide from home page
hideFromHomePage: false
---

# Creating RDDs

There are four ways to create RDDs in Spark.

## Creating RDD from a collection (in-memory)

To create an RDD from a collection, Spark provides two main methods: `parallelize` and `makeRDD`.

```scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
val sparkContext = new SparkContext(sparkConf)
val rdd1 = sparkContext.parallelize(List(1,2,3,4))
val rdd2 = sparkContext.makeRDD(List(1,2,3,4))
rdd1.collect().foreach(println)
rdd2.collect().foreach(println)
sparkContext.stop()
```

In terms of underlying code implementation, `makeRDD` is actually equivalent to `parallelize`.

```scala
def makeRDD[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope {
  parallelize(seq, numSlices)
}
```

## Creating RDD from external storage (files)

Creating an RDD from an external storage system involves local file systems and any Hadoop-supported datasets such as HDFS, HBase, etc.

```scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
val sparkContext = new SparkContext(sparkConf)
val fileRDD: RDD[String] = sparkContext.textFile("input")
fileRDD.collect().foreach(println)
sparkContext.stop()
```

## Creating RDD from another RDD

This method involves generating a new RDD from an existing RDD after performing some operations on it.

## Creating RDD directly (using `new`)

Creating RDD directly using `new` is generally used by the Spark framework itself.

# RDD Parallelism and Partitions

By default, Spark can split a job into multiple tasks and distribute them to executor nodes for parallel processing. The number of tasks that can be processed in parallel is referred to as parallelism. This number can be specified when constructing an RDD. Note that the parallelism here does not refer to the number of tasks being split; do not confuse the two.

```scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
val sparkContext = new SparkContext(sparkConf)
val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4), 4)
val fileRDD: RDD[String] = sparkContext.textFile("input", 2)
fileRDD.collect().foreach(println)
sparkContext.stop()
```

When reading data from memory, the data can be partitioned based on the specified parallelism. The data partitioning rule in Spark's core source code is as follows:

```scala
def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
  (0 until numSlices).iterator.map { i =>
    val start = ((i * length) / numSlices).toInt
    val end = (((i + 1) * length) / numSlices).toInt
    (start, end)
  }
}
```

When reading data from files, the data is split and partitioned based on the rules of Hadoop file reading. The slicing rules differ slightly from the data reading rules. The specific source code in Spark's core is as follows:

```scala
public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
  long totalSize = 0; // compute total size
  for (FileStatus file: files) { // check we have valid files
    if (file.isDirectory()) {
      throw new IOException("Not a file: "+ file.getPath());
    }
    totalSize += file.getLen();
  }
  long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
  long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);

  // ...

  for (FileStatus file: files) {

    // ...

    if (isSplitable(fs, path)) {
      long blockSize = file.getBlockSize();
      long splitSize = computeSplitSize(goalSize, minSize, blockSize);
      // ...
    }
  }

  protected long computeSplitSize(long goalSize, long minSize, long blockSize) {
    return Math.max(minSize, Math.min(goalSize, blockSize));
  }
}
```